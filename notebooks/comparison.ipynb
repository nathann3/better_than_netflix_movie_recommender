{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "comparison.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNafdrccuOSCNr6jfWbqMX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathann3/better_than_netflix_movie_recommender/blob/dev/notebooks/comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEudW5MKLC8E"
      },
      "source": [
        "# Collaborative Filtering Comparison\n",
        "\n",
        "In this notebook we compare different recommendation systems starting with the state-of-the-art LightGCN and going back to the winning algorithm for 2009's Netflix Prize competition, SVD++.\n",
        "\n",
        "Models include in order are LightGCN, NGCF, SVAE, SVD++, and SVD. Each model has their own individual notebooks where we go more indepth, especially LightGCN and NGCF, where we implemented them from scratch in Tensorflow. \n",
        "\n",
        "The last cell compares the performance of the different models using ranking metrics:\n",
        "\n",
        "\n",
        "*   Precision@k\n",
        "*   Recall@k\n",
        "*   Mean Average Precision (MAP)\n",
        "*   Normalized Discounted Cumulative Gain (NDCG)\n",
        "\n",
        "where $k=10$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eESDthIVHdOY"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m10O1R5R8IUO"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "import scipy.sparse as sp\n",
        "import surprise\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from src.data import make_dataset\n",
        "from src.features import build_features\n",
        "from src.models import SVAE, metrics\n",
        "from src.models.GCN import LightGCN, NGCF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c_Xsn7eHgpU"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BTTB4DsY1_Ch",
        "outputId": "d5d0f0c1-c92b-484f-d2d7-1db7f3a3cc67"
      },
      "source": [
        "fp = os.path.join('..', 'data', 'ml-100k.data')\n",
        "make_dataset.download_movie(fp)\n",
        "\n",
        "raw_data = pd.read_csv(fp, sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
        "print(f'Shape: {raw_data.shape}')\n",
        "raw_data.sample(5, random_state=123)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (100000, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42083</th>\n",
              "      <td>600</td>\n",
              "      <td>651</td>\n",
              "      <td>4</td>\n",
              "      <td>888451492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71825</th>\n",
              "      <td>607</td>\n",
              "      <td>494</td>\n",
              "      <td>5</td>\n",
              "      <td>883879556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99535</th>\n",
              "      <td>875</td>\n",
              "      <td>1103</td>\n",
              "      <td>5</td>\n",
              "      <td>876465144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47879</th>\n",
              "      <td>648</td>\n",
              "      <td>238</td>\n",
              "      <td>3</td>\n",
              "      <td>882213535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36734</th>\n",
              "      <td>113</td>\n",
              "      <td>273</td>\n",
              "      <td>4</td>\n",
              "      <td>875935609</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       userId  movieId  rating  timestamp\n",
              "42083     600      651       4  888451492\n",
              "71825     607      494       5  883879556\n",
              "99535     875     1103       5  876465144\n",
              "47879     648      238       3  882213535\n",
              "36734     113      273       4  875935609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e9qxAgkc2mAc",
        "outputId": "c67b258e-cd48-4587-d0b6-a3ea9b80bc4a"
      },
      "source": [
        "# Download movie titles.\n",
        "url = 'https://files.grouplens.org/datasets/movielens/ml-100k/u.item'\n",
        "fp = os.path.join('..', 'data', 'ml-100k.item')\n",
        "r = requests.get(url, stream=True)\n",
        "block_size = 1024\n",
        "total_size = int(r.headers.get('content-length', 0))\n",
        "num_iterables = math.ceil(total_size / block_size)\n",
        "\n",
        "# Download if not already downloaded.\n",
        "if not os.path.exists(fp):\n",
        "    with open(fp, 'wb') as file:\n",
        "        for data in tqdm(\n",
        "            r.iter_content(block_size), total=num_iterables, unit='KB', unit_scale=True\n",
        "        ):\n",
        "            file.write(data)\n",
        "\n",
        "movie_titles = pd.read_csv(fp, sep='|', names=['movieId', 'title'], usecols = range(2), encoding='iso-8859-1')\n",
        "print(f'Shape: {movie_titles.shape}')\n",
        "movie_titles.sample(10, random_state=123)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (1682, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>305</td>\n",
              "      <td>Ice Storm, The (1997)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>451</td>\n",
              "      <td>Grease (1978)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>692</td>\n",
              "      <td>American President, The (1995)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>1409</td>\n",
              "      <td>Swan Princess, The (1994)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>1076</td>\n",
              "      <td>Pagemaster, The (1994)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>104</td>\n",
              "      <td>Theodore Rex (1995)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>168</td>\n",
              "      <td>Monty Python and the Holy Grail (1974)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>1461</td>\n",
              "      <td>Here Comes Cookie (1935)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>1190</td>\n",
              "      <td>That Old Feeling (1997)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>1439</td>\n",
              "      <td>Jason's Lyric (1994)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      movieId                                   title\n",
              "304       305                   Ice Storm, The (1997)\n",
              "450       451                           Grease (1978)\n",
              "691       692          American President, The (1995)\n",
              "1408     1409               Swan Princess, The (1994)\n",
              "1075     1076                  Pagemaster, The (1994)\n",
              "103       104                     Theodore Rex (1995)\n",
              "167       168  Monty Python and the Holy Grail (1974)\n",
              "1460     1461                Here Comes Cookie (1935)\n",
              "1189     1190                 That Old Feeling (1997)\n",
              "1438     1439                    Jason's Lyric (1994)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhdGfZOW24rT",
        "outputId": "17aef20c-64bd-48c5-9516-7c8d49d1da6e"
      },
      "source": [
        "train_size = 0.75\n",
        "train, test = make_dataset.stratified_split(raw_data, 'userId', train_size)\n",
        "\n",
        "print(f'Train Shape: {train.shape}')\n",
        "print(f'Test Shape: {test.shape}')\n",
        "print(f'Do they have the same users?: {set(train.userId) == set(test.userId)}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape: (74992, 4)\n",
            "Test Shape: (25008, 4)\n",
            "Do they have the same users?: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV-Q7up827yC",
        "outputId": "6e8ea578-315d-4b70-d70f-cbb463b3da02"
      },
      "source": [
        "combined = train.append(test)\n",
        "\n",
        "n_users = combined['userId'].nunique()\n",
        "print('Number of users:', n_users)\n",
        "\n",
        "n_movies = combined['movieId'].nunique()\n",
        "print('Number of movies:', n_movies)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users: 943\n",
            "Number of movies: 1682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRzUqVW027v1"
      },
      "source": [
        "# Create DataFrame with reset index of 0-n_movies.\n",
        "movie_new = combined[['movieId']].drop_duplicates()\n",
        "movie_new['movieId_new'] = np.arange(len(movie_new))\n",
        "\n",
        "train_reindex = pd.merge(train, movie_new, on='movieId', how='left')\n",
        "# Reset index to 0-n_users.\n",
        "train_reindex['userId_new'] = train_reindex['userId'] - 1  \n",
        "train_reindex = train_reindex[['userId_new', 'movieId_new', 'rating']]\n",
        "\n",
        "test_reindex = pd.merge(test, movie_new, on='movieId', how='left')\n",
        "# Reset index to 0-n_users.\n",
        "test_reindex['userId_new'] = test_reindex['userId'] - 1\n",
        "test_reindex = test_reindex[['userId_new', 'movieId_new', 'rating']]\n",
        "\n",
        "# Create dictionaries so we can convert to and from indexes\n",
        "item2id = dict(zip(movie_new['movieId'], movie_new['movieId_new']))\n",
        "id2item = dict(zip(movie_new['movieId_new'], movie_new['movieId']))\n",
        "user2id = dict(zip(train['userId'], train_reindex['userId_new']))\n",
        "id2user = dict(zip(train_reindex['userId_new'], train['userId']))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwETtvX27rK",
        "outputId": "d75b5218-7bf0-483c-e0b8-5d1b8c2ba601"
      },
      "source": [
        "# Create user-item graph (sparse matix where users are rows and movies are columns.\n",
        "# 1 if a user reviewed that movie, 0 if they didn't).\n",
        "R = sp.dok_matrix((n_users, n_movies), dtype=np.float32)\n",
        "R[train_reindex['userId_new'], train_reindex['movieId_new']] = 1\n",
        "\n",
        "# Create the adjaceny matrix with the user-item graph.\n",
        "adj_mat = sp.dok_matrix((n_users + n_movies, n_users + n_movies), dtype=np.float32)\n",
        "\n",
        "# List of lists.\n",
        "adj_mat.tolil()\n",
        "R = R.tolil()\n",
        "\n",
        "# Put together adjacency matrix. Movies and users are nodes/vertices.\n",
        "# 1 if the movie and user are connected.\n",
        "adj_mat[:n_users, n_users:] = R\n",
        "adj_mat[n_users:, :n_users] = R.T\n",
        "\n",
        "adj_mat"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2625x2625 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 149984 stored elements in Dictionary Of Keys format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hzlq37t27mv"
      },
      "source": [
        "# Calculate degree matrix D (for every row count the number of nonzero entries)\n",
        "D_values = np.array(adj_mat.sum(1))\n",
        "\n",
        "# Square root and inverse.\n",
        "D_inv_values = np.power(D_values  + 1e-9, -0.5).flatten()\n",
        "D_inv_values[np.isinf(D_inv_values)] = 0.0\n",
        "\n",
        " # Create sparse matrix with the values of D^(-0.5) are the diagonals.\n",
        "D_inv_sq_root = sp.diags(D_inv_values)\n",
        "\n",
        "# Eval (D^-0.5 * A * D^-0.5).\n",
        "norm_adj_mat = D_inv_sq_root.dot(adj_mat).dot(D_inv_sq_root)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjPnmm4l3GDK",
        "outputId": "03de85bc-3741-440a-e536-99b938b3b221"
      },
      "source": [
        "# to COOrdinate format first ((row, column), data)\n",
        "coo = norm_adj_mat.tocoo().astype(np.float32)\n",
        "\n",
        "# create an index that will tell SparseTensor where the non-zero points are\n",
        "indices = np.mat([coo.row, coo.col]).transpose()\n",
        "\n",
        "# covert to sparse tensor\n",
        "A_tilde = tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "A_tilde"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7fee630dba90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuRBZNznJpD6"
      },
      "source": [
        "# Train models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshSoBWYI0rC"
      },
      "source": [
        "## Graph Convoultional Networks (GCNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFRUrT-4HnW1"
      },
      "source": [
        "### Light Graph Convolution Network (LightGCN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7-joA-u3GBB"
      },
      "source": [
        "light_model = LightGCN(A_tilde,\n",
        "                 n_users = n_users,\n",
        "                 n_items = n_movies,\n",
        "                 n_layers = 3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wUU0odC3F8f",
        "outputId": "3b1a94b2-4ec3-4cd6-ddb5-12917cd0fe73"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "light_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "74/74 [==============================] - 3s 32ms/step - training loss: 0.4171\n",
            "Epoch 2/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.2469\n",
            "Epoch 3/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.2234\n",
            "Epoch 4/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.2080\n",
            "Epoch 5/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.1853\n",
            "Epoch 6/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1779\n",
            "Epoch 7/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1721\n",
            "Epoch 8/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.1632\n",
            "Epoch 9/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.1578\n",
            "Epoch 10/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1546\n",
            "Epoch 11/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1484\n",
            "Epoch 12/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.1448\n",
            "Epoch 13/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1437\n",
            "Epoch 14/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1363\n",
            "Epoch 15/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1313\n",
            "Epoch 16/25\n",
            "74/74 [==============================] - 2s 34ms/step - training loss: 0.1285\n",
            "Epoch 17/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1273\n",
            "Epoch 18/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1222\n",
            "Epoch 19/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1172\n",
            "Epoch 20/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1144\n",
            "Epoch 21/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1132\n",
            "Epoch 22/25\n",
            "74/74 [==============================] - 2s 32ms/step - training loss: 0.1086\n",
            "Epoch 23/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1067\n",
            "Epoch 24/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1049\n",
            "Epoch 25/25\n",
            "74/74 [==============================] - 2s 31ms/step - training loss: 0.1003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRp6NESrHqPf"
      },
      "source": [
        "### Neural Graph Collaborative Filtering (NGCF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0q0dyg1zTQP",
        "outputId": "ead49530-dcf6-4710-cb26-07bbe412f9f6"
      },
      "source": [
        "ngcf_model = NGCF(A_tilde,\n",
        "                  n_users = n_users,\n",
        "                  n_items = n_movies,\n",
        "                  n_layers = 3\n",
        "                  )\n",
        "\n",
        "ngcf_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.4326\n",
            "Epoch 2/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.2375\n",
            "Epoch 3/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.2199\n",
            "Epoch 4/25\n",
            "74/74 [==============================] - 3s 35ms/step - training loss: 0.2065\n",
            "Epoch 5/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1934\n",
            "Epoch 6/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1847\n",
            "Epoch 7/25\n",
            "74/74 [==============================] - 2s 34ms/step - training loss: 0.1755\n",
            "Epoch 8/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1710\n",
            "Epoch 9/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1681\n",
            "Epoch 10/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1652\n",
            "Epoch 11/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1628\n",
            "Epoch 12/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1601\n",
            "Epoch 13/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1556\n",
            "Epoch 14/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1535\n",
            "Epoch 15/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1531\n",
            "Epoch 16/25\n",
            "74/74 [==============================] - 3s 35ms/step - training loss: 0.1496\n",
            "Epoch 17/25\n",
            "74/74 [==============================] - 3s 35ms/step - training loss: 0.1434\n",
            "Epoch 18/25\n",
            "74/74 [==============================] - 3s 35ms/step - training loss: 0.1405\n",
            "Epoch 19/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1412\n",
            "Epoch 20/25\n",
            "74/74 [==============================] - 3s 35ms/step - training loss: 0.1385\n",
            "Epoch 21/25\n",
            "74/74 [==============================] - 3s 34ms/step - training loss: 0.1401\n",
            "Epoch 22/25\n",
            "74/74 [==============================] - 2s 34ms/step - training loss: 0.1370\n",
            "Epoch 23/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1336\n",
            "Epoch 24/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1332\n",
            "Epoch 25/25\n",
            "74/74 [==============================] - 2s 33ms/step - training loss: 0.1306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsq8nRDbIqUm"
      },
      "source": [
        "### Recommend with LightGCN and NGCF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_V5VkktAwMh"
      },
      "source": [
        "# Convert test user ids to the new ids\n",
        "users = np.array([user2id[x] for x in test['userId'].unique()])\n",
        "\n",
        "recs = []\n",
        "for model in [light_model, ngcf_model]:\n",
        "    recommendations = model.recommend(users, k=10)\n",
        "    recommendations = recommendations.replace({'userId': id2user, 'movieId': id2item})\n",
        "    recommendations = recommendations.merge(movie_titles,\n",
        "                                                    how='left',\n",
        "                                                    on='movieId'\n",
        "                                                    )[['userId', 'movieId', 'title', 'prediction']]\n",
        "\n",
        "    # Create column with the predicted movie's rank for each user \n",
        "    top_k = recommendations.copy()\n",
        "    top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
        "\n",
        "    recs.append(top_k)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI56qeyiIAQE"
      },
      "source": [
        "## Standard Variational Autoencoder (SVAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7VC52PGVFlD"
      },
      "source": [
        "# Binarize the data (only keep ratings >= 4)\n",
        "df_preferred = raw_data[raw_data['rating'] > 3.5]\n",
        "df_low_rating = raw_data[raw_data['rating'] <= 3.5]\n",
        "\n",
        "df = df_preferred.groupby('userId').filter(lambda x: len(x) >= 5)\n",
        "df = df.groupby('movieId').filter(lambda x: len(x) >= 1)\n",
        "\n",
        "# Obtain both usercount and itemcount after filtering\n",
        "usercount = df[['userId']].groupby('userId', as_index = False).size()\n",
        "itemcount = df[['movieId']].groupby('movieId', as_index = False).size()\n",
        "\n",
        "unique_users =sorted(df.userId.unique())\n",
        "np.random.seed(123)\n",
        "unique_users = np.random.permutation(unique_users)\n",
        "\n",
        "HELDOUT_USERS = 200\n",
        "\n",
        "# Create train/validation/test users\n",
        "n_users = len(unique_users)\n",
        "train_users = unique_users[:(n_users - HELDOUT_USERS * 2)]\n",
        "val_users = unique_users[(n_users - HELDOUT_USERS * 2) : (n_users - HELDOUT_USERS)]\n",
        "test_users = unique_users[(n_users - HELDOUT_USERS):]\n",
        "\n",
        "train_set = df.loc[df['userId'].isin(train_users)]\n",
        "val_set = df.loc[df['userId'].isin(val_users)]\n",
        "test_set = df.loc[df['userId'].isin(test_users)]\n",
        "unique_train_items = pd.unique(train_set['movieId'])\n",
        "val_set = val_set.loc[val_set['movieId'].isin(unique_train_items)]\n",
        "test_set = test_set.loc[test_set['movieId'].isin(unique_train_items)]\n",
        "\n",
        "# Instantiate the sparse matrix generation for train, validation and test sets\n",
        "# use list of unique items from training set for all sets\n",
        "am_train = build_features.AffinityMatrix(df=train_set, items_list=unique_train_items)\n",
        "am_val = build_features.AffinityMatrix(df=val_set, items_list=unique_train_items)\n",
        "am_test = build_features.AffinityMatrix(df=test_set, items_list=unique_train_items)\n",
        "\n",
        "# Obtain the sparse matrix for train, validation and test sets\n",
        "train_data, _, _ = am_train.gen_affinity_matrix()\n",
        "val_data, val_map_users, val_map_items = am_val.gen_affinity_matrix()\n",
        "test_data, test_map_users, test_map_items = am_test.gen_affinity_matrix()\n",
        "\n",
        "# Split validation and test data into training and testing parts\n",
        "val_data_tr, val_data_te = make_dataset.numpy_stratified_split(val_data, ratio=0.75, seed=123)\n",
        "test_data_tr, test_data_te = make_dataset.numpy_stratified_split(test_data, ratio=0.75, seed=123)\n",
        "\n",
        "# Binarize train, validation and test data\n",
        "train_data = np.where(train_data > 3.5, 1.0, 0.0)\n",
        "val_data = np.where(val_data > 3.5, 1.0, 0.0)\n",
        "test_data = np.where(test_data > 3.5, 1.0, 0.0)\n",
        "\n",
        "# Binarize validation data\n",
        "val_data_tr = np.where(val_data_tr > 3.5, 1.0, 0.0)\n",
        "val_data_te_ratings = val_data_te.copy()\n",
        "val_data_te = np.where(val_data_te > 3.5, 1.0, 0.0)\n",
        "\n",
        "# Binarize test data: training part \n",
        "test_data_tr = np.where(test_data_tr > 3.5, 1.0, 0.0)\n",
        "\n",
        "# Binarize test data: testing part (save non-binary version in the separate object, will be used for calculating NDCG)\n",
        "test_data_te_ratings = test_data_te.copy()\n",
        "test_data_te = np.where(test_data_te > 3.5, 1.0, 0.0)\n",
        "\n",
        "# retrieve real ratings from initial dataset \n",
        "test_data_te_ratings=pd.DataFrame(test_data_te_ratings)\n",
        "val_data_te_ratings=pd.DataFrame(val_data_te_ratings)\n",
        "\n",
        "for index,i in df_low_rating.iterrows():\n",
        "    user_old= i['userId'] # old value \n",
        "    item_old=i['movieId'] # old value \n",
        "\n",
        "    if (test_map_users.get(user_old) is not None)  and (test_map_items.get(item_old) is not None) :\n",
        "        user_new=test_map_users.get(user_old) # new value \n",
        "        item_new=test_map_items.get(item_old) # new value \n",
        "        rating=i['rating'] \n",
        "        test_data_te_ratings.at[user_new,item_new]= rating   \n",
        "\n",
        "    if (val_map_users.get(user_old) is not None)  and (val_map_items.get(item_old) is not None) :\n",
        "        user_new=val_map_users.get(user_old) # new value \n",
        "        item_new=val_map_items.get(item_old) # new value \n",
        "        rating=i['rating'] \n",
        "        val_data_te_ratings.at[user_new,item_new]= rating   \n",
        "\n",
        "\n",
        "val_data_te_ratings=val_data_te_ratings.to_numpy()    \n",
        "test_data_te_ratings=test_data_te_ratings.to_numpy()    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM8THbGWVFik",
        "outputId": "5660ebb5-d40c-4b4e-8cb0-06e6a4e41b42"
      },
      "source": [
        "disable_eager_execution()\n",
        "svae_model = SVAE.StandardVAE(n_users=train_data.shape[0],\n",
        "                                   original_dim=train_data.shape[1], \n",
        "                                   intermediate_dim=200, \n",
        "                                   latent_dim=64, \n",
        "                                   n_epochs=400, \n",
        "                                   batch_size=100, \n",
        "                                   k=10,\n",
        "                                   verbose=0,\n",
        "                                   seed=123,\n",
        "                                   drop_encoder=0.5,\n",
        "                                   drop_decoder=0.5,\n",
        "                                   annealing=False,\n",
        "                                   beta=1.0\n",
        "                                   )\n",
        "\n",
        "svae_model.fit(x_train=train_data,\n",
        "          x_valid=val_data,\n",
        "          x_val_tr=val_data_tr,\n",
        "          x_val_te=val_data_te_ratings,\n",
        "          mapper=am_val\n",
        "          )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py:1246: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`model.fit_generator` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK6CVrxqI_Ve"
      },
      "source": [
        "### Recommend with SVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9jsBPW-XQIn"
      },
      "source": [
        "# Model prediction on the training part of test set \n",
        "top_k =  svae_model.recommend_k_items(x=test_data_tr,k=10,remove_seen=True)\n",
        "\n",
        "# Convert sparse matrix back to df\n",
        "recommendations = am_test.map_back_sparse(top_k, kind='prediction')\n",
        "test_df = am_test.map_back_sparse(test_data_te_ratings, kind='ratings') # use test_data_te_, with the original ratings\n",
        "\n",
        "# Create column with the predicted movie's rank for each user \n",
        "top_k = recommendations.copy()\n",
        "top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
        "\n",
        "recs.append(top_k)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJllF7bdJHHN"
      },
      "source": [
        "## Singular Value Decomposition (SVD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzwll5SnITUq"
      },
      "source": [
        "### SVD++"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRc2KJ_8S2W",
        "outputId": "a1f919a9-5f6c-4373-889b-89b6152dc384"
      },
      "source": [
        "surprise_train = surprise.Dataset.load_from_df(train.drop('timestamp', axis=1), reader=surprise.Reader('ml-100k')).build_full_trainset()\n",
        "svdpp = surprise.SVDpp(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
        "svdpp.fit(surprise_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " processing epoch 0\n",
            " processing epoch 1\n",
            " processing epoch 2\n",
            " processing epoch 3\n",
            " processing epoch 4\n",
            " processing epoch 5\n",
            " processing epoch 6\n",
            " processing epoch 7\n",
            " processing epoch 8\n",
            " processing epoch 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x7fee58b07c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_CZ4pCJQef"
      },
      "source": [
        "### SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C52AB0rs-kOu",
        "outputId": "98f23116-63ca-4635-8a5a-35b6e2d8a0fa"
      },
      "source": [
        "svd = surprise.SVD(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
        "svd.fit(surprise_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing epoch 0\n",
            "Processing epoch 1\n",
            "Processing epoch 2\n",
            "Processing epoch 3\n",
            "Processing epoch 4\n",
            "Processing epoch 5\n",
            "Processing epoch 6\n",
            "Processing epoch 7\n",
            "Processing epoch 8\n",
            "Processing epoch 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fee5e47a110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkGR-_YyJSz6"
      },
      "source": [
        "### Recommend with SVD++ and SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLUfa3Xf9vaJ"
      },
      "source": [
        "for model in [svdpp, svd]:\n",
        "    predictions = []\n",
        "    users = train['userId'].unique()\n",
        "    items = train['movieId'].unique()\n",
        "\n",
        "    for user in users:\n",
        "            for item in items:\n",
        "                predictions.append([user, item, model.predict(user, item).est])\n",
        "\n",
        "    predictions = pd.DataFrame(predictions, columns=['userId', 'movieId', 'prediction'])\n",
        "\n",
        "    # Remove movies already seen by users\n",
        "    # Create column of all 1s\n",
        "    temp = train[['userId', 'movieId']].copy()\n",
        "    temp['seen'] = 1\n",
        "\n",
        "    # Outer join and remove movies that have alread been seen (seen=1)\n",
        "    merged = pd.merge(temp, predictions, on=['userId', 'movieId'], how=\"outer\")\n",
        "    merged = merged[merged['seen'].isnull()].drop('seen', axis=1)\n",
        "\n",
        "    # Create filter for users that appear in both the train and test set\n",
        "    common_users = set(test['userId']).intersection(set(predictions['userId']))\n",
        "\n",
        "    # Filter the test and predictions so they have the same users between them\n",
        "    test_common = test[test['userId'].isin(common_users)]\n",
        "    svd_pred_common = merged[merged['userId'].isin(common_users)]\n",
        "\n",
        "    if len(set(merged['userId'])) != len(set(test['userId'])):\n",
        "        print('Number of users in train and test are NOT equal')\n",
        "        print(f\"# of users in train and test respectively: {len(set(merged['userId']))}, {len(set(test['userId']))}\")\n",
        "        print(f\"# of users in BOTH train and test: {len(set(svd_pred_common['userId']))}\")\n",
        "        continue\n",
        "        \n",
        "    # From the predictions, we want only the top k for each user,\n",
        "    # not all the recommendations.\n",
        "    # Extract the top k recommendations from the predictions\n",
        "    top_movies = svd_pred_common.groupby('userId', as_index=False).apply(lambda x: x.nlargest(10, 'prediction')).reset_index(drop=True)\n",
        "    top_movies['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1\n",
        "    \n",
        "    top_k = top_movies.copy()\n",
        "    top_k['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
        "    \n",
        "    recs.append(top_k)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VybicxKFJZXP"
      },
      "source": [
        "# Compare performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5oKqnp_4Q4"
      },
      "source": [
        "Looking at all 5 of our models, we can see that the state-of-the-art model LightGCN vastly outperforms all other models. When compared to SVD++, a widely used algorithm during the Netflix Prize competition, LightGCN achieves an increase in Percision@k by 29%, Recall@k by 18%, MAP by 12%, and NDCG by 35%.\n",
        "\n",
        "NGCF is the older sister model to LightGCN, but only by a single year. We can see how LightGCN improves in ranking metrics compared to NGCF by simply removing unnecessary operations. \n",
        "\n",
        "In conclusion, this demonstrates how far recommendation systems have advanced since 2009, and how new model architectures with notable performance increases can be developed in the span of just 1-2 years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA8JdPhf555b"
      },
      "source": [
        "model_names = ['LightGCN', 'NGCF', 'SVAE', 'SVD++', 'SVD']\n",
        "comparison = pd.DataFrame(columns=['Algorithm', 'Precision@k', 'Recall@k', 'MAP', 'NDCG'])\n",
        "\n",
        "# Convert test user ids to the new ids\n",
        "users = np.array([user2id[x] for x in test['userId'].unique()])\n",
        "\n",
        "for rec, name in zip(recs, model_names):\n",
        "    tester = test_df if name == 'SVAE' else test\n",
        "\n",
        "    pak = metrics.precision_at_k(rec, tester, 'userId', 'movieId', 'rank')\n",
        "    rak = metrics.recall_at_k(rec, tester, 'userId', 'movieId', 'rank')\n",
        "    map = metrics.mean_average_precision(rec, tester, 'userId', 'movieId', 'rank')\n",
        "    ndcg = metrics.ndcg(rec, tester, 'userId', 'movieId', 'rank')\n",
        "\n",
        "    comparison.loc[len(comparison)] = [name, pak, rak, map, ndcg]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ECyE_Qls7KkQ",
        "outputId": "9d886305-655f-4692-eaef-6dcffc20e3b2"
      },
      "source": [
        "comparison"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Algorithm</th>\n",
              "      <th>Precision@k</th>\n",
              "      <th>Recall@k</th>\n",
              "      <th>MAP</th>\n",
              "      <th>NDCG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LightGCN</td>\n",
              "      <td>0.403181</td>\n",
              "      <td>0.214257</td>\n",
              "      <td>0.139248</td>\n",
              "      <td>0.460298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NGCF</td>\n",
              "      <td>0.357264</td>\n",
              "      <td>0.194407</td>\n",
              "      <td>0.117852</td>\n",
              "      <td>0.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVAE</td>\n",
              "      <td>0.356000</td>\n",
              "      <td>0.092862</td>\n",
              "      <td>0.048495</td>\n",
              "      <td>0.354768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SVD++</td>\n",
              "      <td>0.108271</td>\n",
              "      <td>0.038600</td>\n",
              "      <td>0.015655</td>\n",
              "      <td>0.114023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SVD</td>\n",
              "      <td>0.093531</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>0.011672</td>\n",
              "      <td>0.092656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Algorithm  Precision@k  Recall@k       MAP      NDCG\n",
              "0  LightGCN     0.403181  0.214257  0.139248  0.460298\n",
              "1      NGCF     0.357264  0.194407  0.117852  0.405900\n",
              "2      SVAE     0.356000  0.092862  0.048495  0.354768\n",
              "3     SVD++     0.108271  0.038600  0.015655  0.114023\n",
              "4       SVD     0.093531  0.033000  0.011672  0.092656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}